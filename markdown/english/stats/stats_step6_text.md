Plainly put, the larger the sample size, the more likely you will get a significant result. However, it can be impossible or impractical to have a huge sample, you may also be wasting resources and expose more subjects than necessary to risk.

Similarly, if the sample size is too small, then the results might be inconclusive which not only means you will also have wasted time, money, and energy, but it also has ethical implications - you might have exposed human or animal subjects to unnessecary risks as well, even though the results are functionally useless.

When interpreting results in a paper it is important to look at the sample size - while the results are most likely to show a type II error (false negative), that does not mean that any results you _do_ see in a small sample are more reliable.
In fact, small studies often produce unstable estimates, exaggerated effect sizes, and results that fail to replicate. A statistically significant result from a very small sample should be treated with caution, not excitement.

It is very common for studies to use an arbitrary number of samples, for example, n = 10, simply because that is what has been done before, or because it “worked” in a previous paper.

But that is not a scientific justification.

Sample size should be based on:
- The expected effect size
- The variability in the data
- The acceptable risk of Type I error (α)
- The desired statistical power (1 − β)

Using an arbitrary number can lead to underpowered studies that miss real effects, or overpowered studies that detect trivial differences that are not practically meaningful.

Just because a previous study used n = 10 does not mean n = 10 is appropriate for your research question, your design, or your population.

This is why reviewers increasingly expect to see a reported power analysis or a clear rationale for the chosen sample size.

## Power Analysis

A power analysis is made up of four components - if you know, or can estimate three, then you can calculate the fourth.

1. Statistical power: the likelihood that a test will detect an effect of a certain size if there is one, usually set at 80% or higher.
2. Sample size: the minimum number of observations needed to observe an effect of a certain size with a given power level.
3. Significance level (alpha): the maximum risk of rejecting a true null hypothesis that you are willing to take, usually set at 5%.
4. Expected effect size: a standardized way of expressing the magnitude of the expected result of your study, usually based on similar studies or a pilot study.



## Convenience Sample

Sometimes you will see the term "convenience sample", which means participants were selected because they were easy to acceses, such as students from the researcher’s own university or animals already available in a laboratory.

This is not inherently “wrong,” and in many fields it is unavoidable. However, it has important implications. Because:

- Is usually not randomly selected
- May not represent the wider population
- Limits how confidently the results can be generalised