Many mistakenly believe that a small p-value is inherently valuable. As you will have come to realise by now, it is only meaningful if the study has been well-designed and rigorously analysed.

Therefore, some researchers engage in something called p-hacking - rarely maliciously, usually unintentionally. P-hacking is the practice of making analysis decisions in a way that increases the chance of obtaining a statistically significant result.

Examples of p-hacking:

- Checking the results mid-way through collection and stopping once p< 0.05
- Adding more participants after a “nearly significant” result
- Testing multiple outcomes but only reporting the significant ones
- Trying different statistical tests until one gives p < 0.05
- Excluding certain data points after seeing their impact

Any of these things might increase the probabilty of a Type I error. In fact, the more analyses you run, the more likely you are to find something statistically significant purely by chance.

You can reduce the chance of (unintentionally) p-hacking by:

- Pre-registering hypotheses and analysis plans
- Reporting all measured variables and analyses
- Being transparent about exclusions and decisions

The results are the results - better to report your science with integrity, than overinflating the importance of results for a publication.
